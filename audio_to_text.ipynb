{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m95Y9j7ATnUE"
      ],
      "authorship_tag": "ABX9TyMl64Xop64lCWwyU4pFmSAU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fadel11-hub/Tim-Coba-Coba/blob/main/audio_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Audio To Text"
      ],
      "metadata": {
        "id": "kIMmX8qsTqFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Whisper (Open AI)"
      ],
      "metadata": {
        "id": "zUlxvm0_VMrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzYLp0aKhsm4",
        "outputId": "2e74d362-14aa-4c41-9d94-6597edf784bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O93N19E_TwTu",
        "outputId": "9cd9d750-97fe-404a-c770-79d25a844789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Load the Whisper model\n",
        "# This might take some time on the first run as it downloads the model\n",
        "model_whisper = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe the audio file\n",
        "# Make sure you have an audio file named \"audio.mp3\" in your environment\n",
        "result = model_whisper.transcribe(\"/content/drive/MyDrive/Datathon/Tim/Dokumen/Dataset/common_voice_id_25248253.mp3\")\n",
        "\n",
        "# Print the transcribed text\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8UkjAaMTsr-",
        "outputId": "408ba1a7-1f0b-4c2d-c8c4-b71e55b2d759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:01<00:00, 93.3MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ada di lantai 3 bangunan ini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Klasifikasi"
      ],
      "metadata": {
        "id": "dfELPlxpVIAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "Odi-cikxhGcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load CSV dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Datathon/Tim/Dokumen/Dataset/darurat_aman_dataset.csv\")  # Pastikan file ini ada\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "cnducSShhkcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['text'].astype(str).tolist()\n",
        "# Map 'aman' to 0 and 'darurat' to 1\n",
        "label_mapping = {'aman': 0, 'darurat': 1}\n",
        "labels = df['label'].map(label_mapping).astype(int).tolist()"
      ],
      "metadata": {
        "id": "Sy2F1qJnTd10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "data_padded = pad_sequences(sequences, padding='post', truncating='post')\n",
        "\n",
        "print('\\n Word index =', word_index)\n",
        "print('\\n Sequences =', sequences)\n",
        "print('\\n Padded Sequences:')\n",
        "print(data_padded)\n",
        "\n",
        "# # Convert the labels lists into numpy arrays\n",
        "data_labels = np.array(labels)\n",
        "print(data_labels)"
      ],
      "metadata": {
        "id": "TLsE629qPyP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Spilt Train"
      ],
      "metadata": {
        "id": "7m7UZOv9VNmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split # <--- IMPOR PENTING\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data_padded, data_labels, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\n--- Hasil Pembagian Data ---\")\n",
        "print(f\"Shape X_train: {X_train.shape}\")\n",
        "print(f\"Shape y_train: {y_train.shape}\")\n",
        "print(f\"Shape X_test (Validation): {X_test.shape}\")\n",
        "print(f\"Shape y_test (Validation Labels): {y_test.shape}\")"
      ],
      "metadata": {
        "id": "COvCoFCMVNSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Threshold"
      ],
      "metadata": {
        "id": "Eh1CFwqNYk6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of examples to use for training\n",
        "training_size = 2000\n",
        "\n",
        "# Vocabulary size of the tokenizer\n",
        "vocab_size = 1000\n",
        "\n",
        "# Maximum length of the padded sequences\n",
        "max_length = 20\n",
        "\n",
        "# Output dimensions of the Embedding layer\n",
        "embedding_dim = 16\n",
        "\n",
        "# Training data\n",
        "num_epochs = 30"
      ],
      "metadata": {
        "id": "pIkVTWYoX5tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptron"
      ],
      "metadata": {
        "id": "LG0D8FrWMOEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow"
      ],
      "metadata": {
        "id": "Wzup-LOtcb03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    GlobalAveragePooling1D(),  # rata-rata embedding\n",
        "    Dropout(0.5),\n",
        "    Dense(6, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "sLRKYsYeE_77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BTFy8PeHX-Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, verbose=2, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "dsrGu5w7YKqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sckit Learn"
      ],
      "metadata": {
        "id": "PGWZSW6fcgea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n"
      ],
      "metadata": {
        "id": "wZEoQH3ccs_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Misal kolom 'feature1' dan 'feature2' bertipe string\n",
        "label_encoders = {}\n",
        "for col in ['text', 'label']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n"
      ],
      "metadata": {
        "id": "mmg9gkEldQJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_Encoder = LabelEncoder()\n",
        "df['label'] = target_Encoder.fit_transform(df['label'])"
      ],
      "metadata": {
        "id": "rK572VzpdXQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('label', axis=1)\n",
        "y = df['label']"
      ],
      "metadata": {
        "id": "vooiOSqdddj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "XRulQDMldoTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "2sR8zGM3dp_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(100,),  # 1 hidden layer\n",
        "                    max_iter=500,\n",
        "                    activation='relu',\n",
        "                    solver='adam',\n",
        "                     alpha=0.0001,\n",
        "                    random_state=42,)\n",
        "\n",
        "mlp.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "id": "FEeIRj04cjSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = mlp.predict(X_test_scaled)\n",
        "\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "aKh5qZ7Cdxd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save scaler & model\n",
        "# --- Load model dan preprocessing ---\n",
        "mlp = joblib.load(mlp,\"mlp_model.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "label_encoder = joblib.load(\"label_encoder.pkl\")\n"
      ],
      "metadata": {
        "id": "xL0s7hT9elm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "eUpHMQb4rYZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "4Bvmjoc9raBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.fit(X_train, y_train, epochs=num_epochs, verbose=2, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "F1NjPAofBj6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.save(\"model_lstm.h5\")"
      ],
      "metadata": {
        "id": "O5SKsnbBBpN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('tokenizer.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "with open('label_encoder.pickle', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)"
      ],
      "metadata": {
        "id": "H-WfJIcJBsqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model"
      ],
      "metadata": {
        "id": "a0xp__u5kNuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transcribe the audio file\n",
        "# Make sure you have an audio file named \"audio.mp3\" in your environment\n",
        "result2 = model_whisper.transcribe(\"/content/drive/MyDrive/Datathon/Tim/Dokumen/Dataset/Darurat_baru.m4a\")\n",
        "\n",
        "# Print the transcribed text\n",
        "print(result2[\"text\"])"
      ],
      "metadata": {
        "id": "R-L97pDIGi3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Prediksi Teks dari Whisper ---\n",
        "input_seq = tokenizer.texts_to_sequences(result2['text'])\n",
        "input_pad = pad_sequences(input_seq, maxlen=10, padding='post')\n",
        "\n",
        "pred = mlp.predict(input_pad)[0][0]\n",
        "print(\"Status:\", \"ðŸš¨ darurat\" if pred > 0.5 else \"âœ… aman\")"
      ],
      "metadata": {
        "id": "TCFVw7w8P6zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_pred = mlp.predict(result2['text'])\n",
        "print(mlp_pred)\n",
        "print(\"Status:\", \"ðŸš¨ darurat\" if pred == 1 else \"âœ… aman\")"
      ],
      "metadata": {
        "id": "Q9SvfOrVhSh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "eVJTuApTrBQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IMPORT LIBRARY ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# --- LOAD DATASET CSV ---\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Datathon/Tim/Dokumen/Dataset/darurat_aman_dataset.csv\")\n",
        "\n",
        "# âœ… Pisahkan fitur dan label\n",
        "X_text = data[\"text\"]  # kolom teks\n",
        "y = data[\"label\"]     # kolom label\n",
        "\n",
        "# --- ENCODE TEXT KE NUMERIC (PAKAI TOKENIZER) ---\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_text)\n",
        "\n",
        "# Save tokenizer\n",
        "joblib.dump(tokenizer, \"tokenizer.pkl\")\n",
        "\n",
        "X_seq = tokenizer.texts_to_sequences(X_text)\n",
        "X_pad = pad_sequences(X_seq, maxlen=20, padding='post')\n",
        "\n",
        "# --- SCALE FITUR ---\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_pad)\n",
        "\n",
        "# Save scaler\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "# --- ENCODE LABEL ---\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Save label encoder\n",
        "joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
        "\n",
        "# --- SPLIT DATA ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- TRAINING MLP ---\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', max_iter=300)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Save model\n",
        "joblib.dump(mlp, \"mlp_model.pkl\")\n",
        "\n",
        "# --- EVALUASI ---\n",
        "y_pred = mlp.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XHIdPfyieIL",
        "outputId": "bba6edf5-a7e2-4231-81c4-e5485e2c1106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        aman       0.90      0.95      0.92        19\n",
            "     darurat       0.95      0.91      0.93        22\n",
            "\n",
            "    accuracy                           0.93        41\n",
            "   macro avg       0.93      0.93      0.93        41\n",
            "weighted avg       0.93      0.93      0.93        41\n",
            "\n",
            "Akurasi: 0.926829268292683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LOAD MODEL DAN PREPROCESSING ---\n",
        "mlp = joblib.load(\"mlp_model.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
        "tokenizer = joblib.load(\"tokenizer.pkl\")\n",
        "\n",
        "# --- TRANSKRIPSI AUDIO (WHISPER) ---\n",
        "import whisper\n",
        "model_whisper = whisper.load_model(\"base\")\n",
        "\n",
        "result = model_whisper.transcribe(\"/content/drive/MyDrive/Datathon/Tim/Dokumen/Dataset/common_voice_id_25248253.mp3\")\n",
        "result2 = model_whisper.transcribe(\"/content/drive/MyDrive/Datathon/Tim/Dokumen/Dataset/Darurat_baru.m4a\")\n",
        "# print(\"Hasil Transkrip:\", result[\"text\"])\n",
        "\n",
        "input_text = \"saya menonton vidio\"\n",
        "print(\"Input text:\", input_text)\n",
        "\n",
        "# --- PREPROCESS TEKS ---\n",
        "seq = tokenizer.texts_to_sequences(input_text)\n",
        "pad = pad_sequences(seq, maxlen=20, padding='post')\n",
        "scaled = scaler.transform(pad)\n",
        "\n",
        "# --- PREDIKSI KELAS ---\n",
        "pred_encoded = mlp.predict(scaled)[0]\n",
        "pred_label = label_encoder.inverse_transform([pred_encoded])[0]\n",
        "\n",
        "print(\"Prediksi Label (Encoded):\", pred_encoded)\n",
        "print(\"Status:\", \"ðŸš¨ darurat\" if pred_label == \"darurat\" else \"âœ… aman\")\n"
      ],
      "metadata": {
        "id": "S_3l4ZxRkXzV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}